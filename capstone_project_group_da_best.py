# -*- coding: utf-8 -*-
"""Capstone Project - Group Da Best.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mM_jUZX1T4irM7jMS5dsRyN1ru3Nj3Le

# **Data Science Capstone Project Presentation 2025**

**Dataset:** https://www.kaggle.com/code/ahmedabdellahismail/credit-card-customer-segmentation-clustering
"""

#Importing necessary packages

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from sklearn.decomposition import PCA
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

#Reading the file
df = pd.read_csv("2_credit_card_customers.csv", sep=',')

"""Step 1: List Interesting Questions about the Data. Begin by thoroughly examining your dataset. What questions come to mind? What insights do you hope to uncover? List at least 3-5 interesting questions you aim to answer through your analysis. These questions will guide your exploration.

**INITIAL QUESTIONS**
1. How do different types of usage behavior affect customerâ€™s ability to make high percent credit card payments?
2. Optimal amount of credit limit that ensures payment of credit card debt
3. Does balance affect mode of payment? (Cash or installment)
4.  What types of credit card holders are present in the dataset?
- Target high percent full payment and high-value spenders for higher credit limit
- Target high-value spenders for premium offers
- Which group would be most profitable?
5. How many users are inactive? How many used to be high spenders? How many have long tenures?
- Target for retention (rewards loyalty)

Step 2: Perform Exploratory Data Analysis (EDA) Your EDA is crucial for understanding your data before building models. In your notebook, ensure you cover the following:

Inspecting the data
"""

#Viewing the data
df.head()

"""**What does each column represent?**

*   **CUST_ID** - Unique identifier for each Credit Card holder
*   **BALANCE** - Balance amount left to make purchases
*   **BALANCE_FREQUENCY** - How frequently the balance is updated (1 = frequent, 0 = infrequent)
*   **PURCHASES** - Total amount of purchases
*   **ONEOFF_PURCHASES** - Largest single purchase
*   **INSTALLMENTS_PURCHASES** - Total amount of purchases made on an installment plan
*   **CASH_ADVANCE** - Cash in advance given
*   **PURCHASES_FREQUENCY** - How frequently purchases are made (1 = frequent, 0 = infrequent)
*   **ONEOFFPURCHASESFREQUENCY** - How frequently one-time purchases are made (1 = frequent, 0 = infrequent)
*   **PURCHASESINSTALLMENTSFREQUENCY** - How frequently purchases in installments are made (1 = frequent, 0 = infrequent)
*   **CASHADVANCEFREQUENCY** - How frequently the cash in advance being paid
*   **CASHADVANCETRX** - Number of transactions made with "Cash in Advanced"
*   **PURCHASES_TRX** - Number of purchase transactions made
*   **CREDIT_LIMIT** - Maximum amount of credit
*   **PAYMENTS** - Amount of payment done
*   **MINIMUM_PAYMENTS** - Minimum amount of payments made
*   **PRCFULLPAYMENT** - Percent of full payment paid
*   **TENURE** - Years of credit card service
"""

#Subitem 1
#Understand and Describe the Dataset: Provide a clear overview of the dataset.

print("Data Dimensions:")
df.shape

print("Data Info:")
df.info()

print("Data Summary:")
df.describe()

print("Checking for duplicates:")
print("Duplicate rows:",df.duplicated().sum())
print("Duplicate customers:",len(df)-len(df['CUST_ID'].unique()))

print("Checking the data for missing values:")
df.isnull().sum()

#Subitem 2
#Describe the Distribution of the Dataset.
#Analyze the distributions of individual variables.
#Use appropriate visualizations (histograms, box plots, etc.) to show their spread, central tendency, and shape.

#We do not include the Customer ID in checking the distribution.

fig, axes = plt.subplots(3, 6, figsize=(20, 10))
axes = axes.flatten()

for i in range(1, len(df.columns)):
    sns.histplot(df[df.columns[i]], kde=True, ax=axes[i-1])
    axes[i-1].set_title(df.columns[i])

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(3, 6, figsize=(20, 10))
axes = axes.flatten()

for i in range(1,len(df.columns)):
    axes[i-1].boxplot(df[df.columns[i]])
    axes[i-1].set_title(df.columns[i])

plt.tight_layout()
plt.show()

"""*From the boxplots, we can see a lot points beyond the upper and lower bounds, indicating the presence of outliers in the data.*

Cleaning the data
"""

#Since we already know there are no repeated customer ID's, we can drop the column as it is only a unique identifier.
df.drop(['CUST_ID'],axis=1,inplace=True)

#Identifying and converting outliers to NaN

for i in range(1,len(df.columns)):
  P25 = df[df.columns[i]].quantile(0.25)
  P75 = df[df.columns[i]].quantile(0.75)
  IQR = P75 - P25
  lower_bound = P25 - 1.5*IQR
  upper_bound = P75 + 1.5*IQR
  outliers = ((df[df.columns[i]] < lower_bound) |(df[df.columns[i]] > upper_bound))
  df.loc[outliers, df.columns[i]] = np.nan

#Subitem 3
#Explore and Handle Missing Values: Identify any missing data points.
#Describe your strategy for handling them (e.g., imputation, removal) and explain your reasoning.

#Subitem 5
#Report Outliers:
#Identify and discuss any outliers present in your data.
#Explain their potential impact and how you decided to address them (if at all).

print("Checking for the number of NaN values after removing outliers:")
df.isnull().sum()

#We replace NaN values with the median
df = df.fillna(df.median())

#The distribution in some features appear to be bimodal (most entries are located on the extremes).
#Treating these values as outliers then imputing using the median value may not make sense since the median values are less represented.
#An alternative approach is to check if groups with different spending profiles exist.

#Subitem 4
#Describe Relationships Between Variables:
#Investigate how different variables relate to each other.
#Use correlation matrices, scatter plots, or other suitable visualizations to illustrate these relationships.
#Pairplots and heatmap

#Since we are doing removing outliers, we can now inspect the relationship between variables
df_numerical = df.select_dtypes(include=['float64', 'int64']).drop(columns=['TENURE']) # Exclude TENURE as it has limited values
plt.figure(figsize=(12, 10))
correlation_matrix = df_numerical.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()

sns.pairplot(df, hue='PURCHASES_FREQUENCY',vars=['BALANCE','PURCHASES','CREDIT_LIMIT','PAYMENTS','MINIMUM_PAYMENTS'],palette="rainbow")
plt.show()

#Subitem 6
#Perform Feature Engineering (as deemed necessary):
#Create new features from existing ones if you believe they will improve your models.
#Explain the rationale behind your new features.

#We can add a new feature that shows the average price of each purchase by dividing the purchases by the number of purchase transactions

df['AVG_TRX_AMOUNT'] = df['PURCHASES'] / df['PURCHASES_TRX']
df.fillna({'AVG_TRX_AMOUNT': 0}, inplace=True)

"""*Replace NaN with 0's. These are rows wherein there were no purchases and/or purchase transactions. It is possible that they got a "free item," which would result to a 0 PURCHASE but a >0 PURCHASE_TRX. It is right to change this to 0.*"""

infinite_indices = df[np.isinf(df['AVG_TRX_AMOUNT'])].index
print("Indexes with infinite values in 'AVG_TRX_AMOUNT':", infinite_indices)

df['AVG_TRX_AMOUNT'] = df['AVG_TRX_AMOUNT'].replace([np.inf, -np.inf], np.nan)
df.dropna(subset=['AVG_TRX_AMOUNT'], inplace=True)

"""*It doesn't make sense that there is a purchase but no recorded transaction. The three inf values are dropped instead.*"""

#Subitem 7
#Prepare for Data Modeling:
#Discuss any necessary preprocessing steps taken to prepare your data for modeling (e.g., scaling, encoding categorical variables).

df_transformed = pd.DataFrame(StandardScaler().fit_transform(df),columns=df.columns)
df_transformed

pca = PCA(n_components=18)
pca.fit(df_transformed)

variance = pca.explained_variance_ratio_
var = np.cumsum(np.round(variance,3)*100)
plt.figure(figsize=(12,6))
plt.ylabel('%variance Explained')
plt.xlabel('# of Features')
plt.title('PCA Analysis')
plt.ylim(0,100.5)
plt.yticks(np.arange(0,101,10))
plt.xticks(np.arange(0,18,1))
plt.grid()
plt.plot(var)

"""*We want an explained variance of at least 60%. We will choose 11 which is about 90%*"""

pca = PCA(n_components=11)
pca_scale = pca.fit_transform(df_transformed)
pca_df_scale = pd.DataFrame(pca_scale, columns=['pc1','pc2','pc3','pc4','pc5','pc6','pc7','pc8','pc9','pc10','pc11'])
pca_df_scale.head()

"""Step 3: Define an Unsupervised Learning Goal and Perform Unsupervised Learning. Report the Results. Choose an appropriate unsupervised learning task based on your data and the questions you listed in Step 1. This could involve clustering, dimensionality reduction, or association rule mining. Clearly define your goal and implement your chosen algorithm. Report and interpret your results, explaining what insights you gained."""

#We use the elbow method to identify the optimal number of clusters to use
from yellowbrick.cluster import KElbowVisualizer

pca_model = KMeans()
visualizer = KElbowVisualizer(pca_model, k=(2,10))

visualizer.fit(pca_df_scale)
visualizer.show()

"""*From the results, 4 clusters is the optimal number.*"""

clusterNum = 4
k_means_pca = KMeans(init = "k-means++", n_clusters = clusterNum, n_init = 12)
k_means_pca.fit(pca_df_scale)
Clusters_temp = k_means_pca.labels_
CL = pd.DataFrame({'Clusters': Clusters_temp})
CL['Clusters'].unique()
Clusters = np.full(len(CL),np.nan)
for j in range(0,clusterNum):
  Cluster_val = Clusters_temp[[CL.loc[CL['Clusters']==i].index[0] for i in CL['Clusters'].unique()][j]]
  for k in range(len(Clusters_temp)):
    if Clusters_temp[k] == Cluster_val:
      Clusters[k] = int(j)

pca_df_scale['Cluster'] = Clusters
df_transformed['Cluster'] = Clusters
pca_df_scale[0:6]

#Checking the distribution of customers across clusters
sns.histplot(pca_df_scale['Cluster'], bins=4)
plt.xlabel('Cluster')
plt.ylabel('Count')
plt.xticks([0, 1, 2, 3])
plt.title('Distribution of Customers Across Clusters')
plt.show()

x_axis = pca_df_scale['pc2']
y_axis = pca_df_scale['pc1']
plt.figure(figsize=(10,8))
sns.scatterplot(x=x_axis, y=y_axis, hue = pca_df_scale['Cluster'], palette = ['green','red','purple','yellow'])
plt.title('clusters by pca')
plt.show()

import plotly.express as px

polar=df_transformed.groupby("Cluster").mean().reset_index()
polar=pd.melt(polar,id_vars=["Cluster"])
#fig = px.line_polar(polar, r="value", theta="variable", color="Cluster", color_discrete_sequence=['green','red','purple','yellow'], line_close=True,height=600,width=1200)
fig = px.line_polar(polar, r="value", theta="variable", color="Cluster", color_discrete_sequence=['white','white','white','yellow'],line_dash="Cluster",line_dash_sequence=['dot','dot','dot','solid'], line_close=True,height=600,width=1200)
fig.show()

"""From the graph above, we can visually see the characteristics of each cluster:

**Denied:** Among the 4, this is the most uninteresting cluster characterized by users who hardly utilize their credit cards.

**For Review 1:** This cluster of customers utilize their credit cards to do cash advances. The cash they used "withdrew" is what they used to make purchases, so we cannot get the data for their purcahses and purchase transactions. They are probably doing this so they can limit their expenses per month which would explain why they have the highest balance among the four clusters.

**Approved:** Similar to the previous cluster, this cluster also loves to make many purchases, but most of them are single-payment purchases. With high amounts of purchases, they also do high payments to cover them.

**For Review 2:** This cluster of customers like to frequently make purchases through installments
"""

cluster_names = {
    0.0: 'Denied',
    1.0: 'For Review 1',
    2.0: 'Approved',
    3.0: 'For Review 2'
}
pca_df_scale['Cluster'] = pca_df_scale['Cluster'].map(cluster_names)
df_transformed['Cluster'] = df_transformed['Cluster'].map(cluster_names)
df_transformed

"""Step 4: Define a Supervised Learning Goal and Perform Supervised Learning. Report Your Results. Formulate a supervised learning problem (classification or regression) that your data can address. Clearly state your goal (e.g., ""predict customer churn,"" ""classify email as spam""). Implement at least one supervised learning algorithm, train your model, and evaluate its performance using appropriate metrics. Report your results, including model performance metrics and a discussion of your findings.

"""

df_transformed

X = df_transformed.iloc[:, 0:17]
y = df_transformed['Cluster']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=85)
model_svm = SVC(kernel='rbf')
model_svm.fit(X_train, y_train)
y_pred_svm = model_svm.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred_svm))
print(confusion_matrix(y_test, y_pred_svm))
print(classification_report(y_test, y_pred_svm))
sns.heatmap(confusion_matrix(y_test, y_pred_svm), annot=True, fmt='d', cmap='Purples', xticklabels=['Approved', 'Denied', 'For Review 1', 'For Review 2'], yticklabels=['Approved', 'Denied', 'For Review 1', 'For Review 2'])
plt.title("Confusion Matrix - SVM")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

#maybe add an additional feature such as credit_pct = BALANCE/CREDIT_LIMIT .
#Customers with high balances relative to their credit limit are more likely to be high-risk(the more that they use their credit limit means more financial stress)?

#can also consider payment irregularity score: payments/(purchases + cash advance)
#also low score means a customer spends more than they are paying

# # df_transformed

# # Calculated Features
# ## computing the credit score
# # 'CREDIT_SCORE' = 35%'PRC_FULL_PAYMENT' + 30%'CREDIT_USE' + 15%'TENURE' + 10%'PAYMENT_RATIO' + 10%'CASH_ADV_RATIO'
# # PRC_Full_Payment indicates payment consistency (payment history)


# df_supervised = df_transformed
# # Amount of credit used by customer. Nearly/maxing out the credit limit indicates a lower chance of customer paying
# df_supervised['CREDIT_USE'] = df['BALANCE'] / df['CREDIT_LIMIT']

# # Tenure indicates longevity of the user; Higher tenure indicates a more stable and trustworthy customer
# df_supervised['TENURE_LENGTH'] = df['TENURE']/df['TENURE'].max()

# # low payments large purchases indicates a lower chance of the customer paying;
# # high payments low purchases indicates a higher chance of the customer paying
# # the higher the payment_ratio, the better.
# df_supervised['PAYMENT_RATIO'] = df['PAYMENTS'] / (df['PURCHASES'] + 0.0001)
# #
# # High cash_advance low balance indicates a lower chance of the customer paying;
# # low cash_advance high balance indicates a higher chance of the customer paying
# # the lower the cash_adv_ratio, the better.
# df_supervised['CASH_ADV_RATIO'] = df['CASH_ADVANCE'] / (df['BALANCE'] + 0.0001)
# print(df_supervised.isna().sum())
# # scaling the dataset
# from sklearn.preprocessing import MinMaxScaler

# scaler = MinMaxScaler()
# orig_data = ['BALANCE', 'CREDIT_LIMIT', 'PAYMENTS', 'PURCHASES', 'CASH_ADVANCE']
# features= ['CREDIT_USE', 'PAYMENT_RATIO', 'CASH_ADV_RATIO', 'PRC_FULL_PAYMENT']
# df_scaled_supervised = pd.DataFrame(scaler.fit_transform(df_supervised[features]), columns = features)
# df_scaled_supervised['TENURE_LENGTH'] = df_supervised['TENURE_LENGTH']
# df_scaled_supervised['CREDIT_SCORE'] = 35*df_scaled_supervised['PRC_FULL_PAYMENT'] + (1 - 30*df_scaled_supervised['CREDIT_USE']) +  15*df_scaled_supervised['TENURE_LENGTH'] + 10*df_scaled_supervised['PAYMENT_RATIO'] + (1 - 10*df_scaled_supervised['CASH_ADV_RATIO'])
# df_scaled_supervised['CREDIT_SCORE'] = 300 + (df_scaled_supervised['CREDIT_SCORE']/100)*850 #scale the score from 300=850

# criterion = df_scaled_supervised['CREDIT_SCORE'].median()

# sns.histplot(data=df_scaled_supervised, x='CREDIT_SCORE', kde = True)
# plt.axvline(criterion, color = 'red', linestyle='--' )
# plt.tight_layout()
# plt.show()

# print(df_scaled_supervised.isna().sum())
# df_scaled_supervised[df_scaled_supervised.isnull().any(axis=1)]

# df_scaled_supervised['RATING'] = df_scaled_supervised['CREDIT_SCORE'].apply(lambda x: 1 if x >= criterion else 0)
# sns.histplot(data=df_scaled_supervised, x='CREDIT_SCORE', hue = 'RATING')
# plt.axvline(criterion, color = 'red', linestyle='--' )
# plt.tight_layout()
# plt.show()

# df_scaled_supervised[orig_data] = df[orig_data]
# df_scaled_supervised.shape

# from sklearn.svm import SVC
# df_scaled_supervised = df_scaled_supervised.dropna()
# df_scaled_supervised = pd.get_dummies(df_scaled_supervised, drop_first=True)
# x = df_scaled_supervised.drop('RATING', axis=1)
# y = df_scaled_supervised['RATING']
# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
# print(x_train.shape, x_test.shape)

# scaler = StandardScaler()
# x_train_scaled = scaler.fit_transform(x_train)
# x_test_scaled = scaler.transform(x_test)
# model = SVC(kernel='rbf')
# model.fit(x_train_scaled, y_train)
# y_pred = model.predict(x_test_scaled)
# print(confusion_matrix(y_test, y_pred))
# print(classification_report(y_test, y_pred))
# # pretty way to visualize confusion matrix
# sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Purples')
# plt.title("Confusion Matrix - SVM")
# plt.xlabel("Predicted")
# plt.ylabel("Actual")
# plt.show()

from sklearn.preprocessing import StandardScaler
import numpy as np

def predict_cluster(balance, balance_frequency, purchases, oneoff_purchases, installments_purchases,
                    cash_advance, purchases_frequency, oneoff_purchases_frequency, installments_purchases_frequency,
                    cash_advance_frequency, cash_advance_trx, purchases_trx, credit_limit, payments,
                    minimum_payments, prc_full_payment, tenure):
    """
    Predicts the customer cluster based on input features.

    Args:
        balance (float): Balance amount left to make purchases.
        balance_frequency (float): How frequently the balance is updated (1 = frequent, 0 = infrequent).
        purchases (float): Total amount of purchases.
        oneoff_purchases (float): Largest single purchase.
        installments_purchases (float): Total amount of purchases made on an installment plan.
        cash_advance (float): Cash in advance given.
        purchases_frequency (float): How frequently purchases are made (1 = frequent, 0 = infrequent).
        oneoff_purchases_frequency (float): How frequently one-time purchases are made (1 = frequent, 0 = infrequent).
        installments_purchases_frequency (float): How frequently purchases in installments are made (1 = frequent, 0 = infrequent).
        cash_advance_frequency (float): How frequently the cash in advance being paid.
        cash_advance_trx (int): Number of transactions made with "Cash in Advanced".
        purchases_trx (int): Number of purchase transactions made.
        credit_limit (float): Maximum amount of credit.
        payments (float): Amount of payment done.
        minimum_payments (float): Minimum amount of payments made.
        prc_full_payment (float): Percent of full payment paid.
        tenure (float): Years of credit card service.

    Returns:
        str: Predicted cluster name.
    """
    # Create a DataFrame from the input values
    input_data = pd.DataFrame([[balance, balance_frequency, purchases, oneoff_purchases, installments_purchases,
                                cash_advance, purchases_frequency, oneoff_purchases_frequency, installments_purchases_frequency,
                                cash_advance_frequency, cash_advance_trx, purchases_trx, credit_limit, payments,
                                minimum_payments, prc_full_payment, tenure]],
                              columns=['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES',
                                       'CASH_ADVANCE', 'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY',
                                       'CASH_ADVANCE_FREQUENCY', 'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',
                                       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE'])

    # Apply the same scaling used during training
    input_scaled = StandardScaler().fit_transform(input_data)

    # Make the prediction
    prediction = model_svm.predict(input_scaled)[0]

    return prediction

import ipywidgets as widgets
import warnings
from IPython.display import display
warnings.filterwarnings("ignore")

# Create widgets for each feature with adjusted layout
widget_layout = widgets.Layout(description_width='500px', width='500px')

balance_widget = widgets.FloatText(description='BALANCE:', layout=widget_layout)
balance_frequency_widget = widgets.FloatText(description='BALANCE FREQUENCY:', layout=widget_layout)
purchases_widget = widgets.FloatText(description='PURCHASES:', layout=widget_layout)
purchases_frequency_widget = widgets.FloatText(description='PURCHASES FREQUENCY:', layout=widget_layout)
purchases_trx_widget = widgets.IntText(description='PURCHASES TRX:', layout=widget_layout)
oneoff_purchases_widget = widgets.FloatText(description='ONEOFF PURCHASES:', layout=widget_layout)
oneoff_purchases_frequency_widget = widgets.FloatText(description='ONEOFF PURCHASES FREQUENCY:', layout=widget_layout)
installments_purchases_widget = widgets.FloatText(description='INSTALLMENTS PURCHASES:', layout=widget_layout)
installments_purchases_frequency_widget = widgets.FloatText(description='INSTALLMENTS PURCHASES FREQUENCY:', layout=widget_layout)
cash_advance_widget = widgets.FloatText(description='CASH ADVANCE:', layout=widget_layout)
cash_advance_frequency_widget = widgets.FloatText(description='CASH ADVANCE FREQUENCY:', layout=widget_layout)
cash_advance_trx_widget = widgets.IntText(description='CASH ADVANCE TRX:', layout=widget_layout)
credit_limit_widget = widgets.FloatText(description='CREDIT LIMIT:', layout=widget_layout)
payments_widget = widgets.FloatText(description='PAYMENTS:', layout=widget_layout)
minimum_payments_widget = widgets.FloatText(description='MINIMUM PAYMENTS:', layout=widget_layout)
prc_full_payment_widget = widgets.FloatText(description='PERCENT FULL PAYMENT:', layout=widget_layout)
tenure_widget = widgets.FloatText(description='TENURE:', layout=widget_layout)


output_widget = widgets.Output()

# Button to trigger prediction
predict_button = widgets.Button(description='Assess')

def on_predict_button_clicked(b):
    with output_widget:
        output_widget.clear_output()
        predicted_cluster = predict_cluster(
            balance=balance_widget.value,
            balance_frequency=balance_frequency_widget.value,
            purchases=purchases_widget.value,
            oneoff_purchases=oneoff_purchases_widget.value,
            installments_purchases=installments_purchases_widget.value,
            cash_advance=cash_advance_widget.value,
            purchases_frequency=purchases_frequency_widget.value,
            oneoff_purchases_frequency=oneoff_purchases_frequency_widget.value,
            installments_purchases_frequency=installments_purchases_frequency_widget.value,
            cash_advance_frequency=cash_advance_frequency_widget.value,
            cash_advance_trx=cash_advance_trx_widget.value,
            purchases_trx=purchases_trx_widget.value,
            credit_limit=credit_limit_widget.value,
            payments=payments_widget.value,
            minimum_payments=minimum_payments_widget.value,
            prc_full_payment=prc_full_payment_widget.value,
            tenure=tenure_widget.value
        )
        print(f"Assessment: {predicted_cluster}")

predict_button.on_click(on_predict_button_clicked)

# Arrange widgets in a VBox
input_widgets = widgets.VBox([
    balance_widget,
    balance_frequency_widget,
    purchases_widget,
    purchases_frequency_widget,
    purchases_trx_widget,
    oneoff_purchases_widget,
    oneoff_purchases_frequency_widget,
    installments_purchases_widget,
    installments_purchases_frequency_widget,
    cash_advance_widget,
    cash_advance_frequency_widget,
    cash_advance_trx_widget,
    credit_limit_widget,
    payments_widget,
    minimum_payments_widget,
    prc_full_payment_widget,
    tenure_widget,
    predict_button,
    output_widget
])

display(input_widgets)

